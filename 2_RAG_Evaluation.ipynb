{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1c8f14-09ee-44f4-8a22-8f82b5a8d314",
   "metadata": {},
   "source": [
    "# Run the following if you are on Colab\n",
    "\n",
    "- change the resource type to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305bea04-fe3e-4e06-86b9-1cec0a61c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/scbxtraining/scbx-rag.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d79016-1b8d-486c-8351-288e98b72ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/scbx-rag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb16fc6-ce72-49fa-85fc-607b11659c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df6d4f-871f-42e5-baa8-027918c8851d",
   "metadata": {},
   "source": [
    "# RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20516d-10dd-40b2-8e9f-5530f2023c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257914f7-70a8-4ff7-ad84-1d26a67acae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"OPENAI_KEY\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"OPENAI_ENDPOINT\"\n",
    "deployment_name=\"DEPLOYMENT_NAME\"\n",
    "api_version=\"API_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb3c6df-6533-4f24-b826-480009c48af4",
   "metadata": {},
   "source": [
    "### What is RAGAs\n",
    "\n",
    "RAGAs (Retrieval-Augmented Generation Assessment) is a framework that provides you with the necessary ingredients to help you evaluate your RAG pipeline on a component level.\n",
    "\n",
    "### Evaluation Data\n",
    "To evaluate the RAG pipeline, RAGAs expects the following information:\n",
    "\n",
    "1. question: The user query that is the input of the RAG pipeline. The input.\n",
    "\n",
    "2. answer: The generated answer from the RAG pipeline. The output.\n",
    "\n",
    "3. contexts: The contexts retrieved from the external knowledge source used to answer the question.\n",
    "\n",
    "4. ground_truths: The ground truth answer to the question. This is the only human-annotated information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534098a3-8b26-4249-9c5c-29c743d39dce",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "RAGAs provide you with a few metrics to evaluate a RAG pipeline component-wise as well as end-to-end.\n",
    "\n",
    "1. Context precision: measures the signal-to-noise ratio of the retrieved context. This metric is computed using the question and the contexts.\n",
    "\n",
    "2. Context recall: measures if all the relevant information required to answer the question was retrieved. This metric is computed based on the ground_truth (this is the only metric in the framework that relies on human-annotated ground truth labels) and the contexts.\n",
    "\n",
    "3. Faithfulness: measures the factual accuracy of the generated answer. The number of correct statements from the given contexts is divided by the total number of statements in the generated answer. This metric uses the question, contexts and the answer.\n",
    "\n",
    "4. Answer relevancy: measures how relevant the generated answer is to the question. This metric is computed using the question and the answer. For example, the answer “France is in western Europe.” to the question “Where is France and what is it’s capital?” would achieve a low answer relevancy because it only answers half of the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c3f14-13c4-4475-8bbe-002f8258e916",
   "metadata": {},
   "source": [
    "![title](./imgs/ragas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e0bc7-1610-4300-950d-a3305c1d23ff",
   "metadata": {},
   "source": [
    "## Setting up the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbbdc4c-6995-404d-907c-de73cee303e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93348dba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=deployment_name,\n",
    "    api_version=api_version, \n",
    "    temperature=0,\n",
    "    max_tokens=2000,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3e703-402c-4ee9-b5a5-3ea13c71f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_RAG_retrieval(input_pdf_path = './inputs/'):\n",
    "    docs = []\n",
    "    files = os.listdir(input_pdf_path)\n",
    "    files = [x for x in files if x.endswith('.pdf')]\n",
    "    \n",
    "    for file in files:\n",
    "        loader = PyMuPDFLoader(f\"{input_pdf_path}/{file}\")\n",
    "        doc = loader.load()\n",
    "        for _ in doc:\n",
    "            additional_metadata = {\n",
    "                                    \"last_modified_date\": file.split('.')[0].split('_')[1],\n",
    "                                    \"document_name\": file.split('.')[0].split('_')[0],\n",
    "                                }\n",
    "            _.metadata.update(additional_metadata)\n",
    "    \n",
    "        docs = docs + doc\n",
    "    \n",
    "    # Chucking: Split the text into chunks\n",
    "    CHUNK_SIZE = 4000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    print(f\"splitted texts with length: {len(texts)}\")    \n",
    "    vectorstore = Chroma.from_documents(documents=texts[:], embedding=embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8561182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_rag(retriever):\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "        Use the following context (delimited by <ctx></ctx>) to answer the question. \n",
    "        Use the context to provide the answer only. \n",
    "        ------\n",
    "        <ctx>\n",
    "        {context}\n",
    "        </ctx>\n",
    "        ------\n",
    "        {question}\n",
    "        Answer:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    custom_rag_prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | custom_rag_prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679232e-b1ad-4ec3-aa29-a6204d290076",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retrieval = init_RAG_retrieval(\"./inputs/\")\n",
    "rag_chain = create_custom_rag(rag_retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b670695-748e-4ed9-a3a2-1390c19aaa5b",
   "metadata": {},
   "source": [
    "## Preparing the Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12205ff4-b773-484b-bb94-e0061626f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the reduction in scope 1 and scope 2 emissions that SCB achieved in 2023?\", \n",
    "    \"What share of SCBX's total revenue was powered by AI in 2023?\", \n",
    "    \"What is SCBX's 2025 financial support target for  'Net Zero financed emissions' for scope 3 emissions?\", \n",
    "    \"Can you summarize what SCBX is doing to improve Financial and digital literacy\", \n",
    "    \"What has been Thailland's share of economic loss from extreme climate events between 2000 to 2019?\", \n",
    "    \"What are SCBX's scope 1 and 2 emissions for year 2023 and how much reduction have we seen from the year before?\", \n",
    "    \"What is sCBX's scope 3 emissions baseline, which year was it measured in?\"\n",
    "]\n",
    "\n",
    "\n",
    "ground_truth = [\n",
    "    \"SCB achieved a 7% reduction in scope 1 and scope 2 emissions in 2023.\", \n",
    "    \"To check where to get the answer\", \n",
    "    \"Baht 200,000 million\", \n",
    "    \"SCBX Group is committed to nurturing digital skills and promoting technological digital literacy in society to propel long-term economic growth, broaden employment opportunities, and uplift people's quality of life. They leverage technology to enhance financial solutions, drive innovation, and accelerate financial inclusion across all user groups. Additionally, SCBX collaborates with leading partners to broaden financial service channels, promote job and income opportunities, and nurture financial literacy and discipline.\", \n",
    "    \"Thailand was confronted with 146 extreme weather events, resulting in an economic loss of around USD 7.7 billion between 2000 to 2019.\", \n",
    "    \"SCBX's scope 1 and 2 emissions for the year 2023 amounted to 65,384 tonnes of carbon dioxide equivalent, achieving a targeted 7% reduction compared to 2022.\", \n",
    "    \"SCBX's scope 3 emissions baseline was measured in 2021, and the top 3 sectors that fall under this category are the power sector, energy (fossil fuel), and hospitality & real estate.\"\n",
    "]\n",
    "\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# Inference\n",
    "for query in questions:\n",
    "  answers.append(rag_chain.invoke(query))\n",
    "  contexts.append([docs.page_content for docs in rag_retrieval.get_relevant_documents(query)])\n",
    "\n",
    "# To dict\n",
    "dataset = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truth\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "# dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae9151-6e1e-4fa2-8d12-ca5ea200ba4a",
   "metadata": {},
   "source": [
    "# Evaluating the Reusts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85df167-6a62-4ac0-af62-2b90bfc21fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c3ce2-a906-406b-ba91-14eb3b6c9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece398ef-91aa-4650-a69f-bbb872ab7bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284cca75-4550-463c-b0d2-f47a83bb73b1",
   "metadata": {},
   "source": [
    "1. context_relevancy (signal-to-noise ratio of the retrieved context): While the LLM judges all of the context as relevant for the last question, it also judges that most of the retrieved context for the second question is irrelevant. Depending on this metric, you could experiment with different numbers of retrieved contexts to reduce the noise.\n",
    "2. context_recall (if all the relevant information required to answer the question was retrieved): The LLMs evaluate that the retrieved contexts contain the relevant information required to answer the questions correctly.\n",
    "3. faithfulness (factual accuracy of the generated answer): While the LLM judges that the first and last questions are answered correctly, the answer to the second question, which wrongly states that the president did not mention Intel’s CEO, is judged with a faithfulness of 0.5.\n",
    "4. answer_relevancy (how relevant is the generated answer to the question): All of the generated answers are judged as fairly relevant to the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea48a8-b607-419b-8ab8-d532045d5e6b",
   "metadata": {},
   "source": [
    "## QA Evaluation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98e05d-c9b4-42a7-ade0-3e7c1b0d78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e14283d-e34d-4423-8aa2-7a92f05b719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "predictions = []\n",
    "\n",
    "for question, truth, prediction in zip(questions, ground_truth, answers):\n",
    "    examples.append(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            # Ground truth\n",
    "            \"answer\": real_answer\n",
    "        }\n",
    "    )\n",
    "    predictions.append({'response': prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7bbc11-d8bd-4e6e-9303-6bc74f06d16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "    You are grading the following question:\n",
    "    {query}\n",
    "    Here is the real answer:\n",
    "    {answer}\n",
    "    You are grading the following predicted answer:\n",
    "    {result}\n",
    "    What grade do you give from 0 to 10, where 0 is the lowest (very low similarity) and 10 is the highest (very high similarity)?\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155f311-65ae-4619-be1d-87ee02495b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "evalchain = QAEvalChain.from_llm(llm=llm, prompt=PROMPT)\n",
    "\n",
    "eval_result = evalchain.evaluate(\n",
    "    examples,\n",
    "    predictions,\n",
    "    question_key=\"question\",\n",
    "    answer_key=\"answer\",\n",
    "    prediction_key=\"response\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ff5dc-ccf7-4183-9455-e4fc5b46126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36173e9-0b98-42f6-b52e-7f24c78d4e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
