{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376ce9b8-c3a3-4bd2-953e-af3f8d98d073",
   "metadata": {},
   "source": [
    "# Run the following if you are on Colab\n",
    "\n",
    "- change the resource type to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29d940-f0eb-4494-b764-262f1bdcaa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/scbxtraining/scbx-rag.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7061ad7-18d2-4d6f-bbb6-70ebbf4b6368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/scbx-rag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9743c3-efa2-445b-a190-de065d976966",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1160a4ff",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0085d-aa36-43c8-97b5-98cbe3988f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e0d1da-8904-40f9-b12f-39b40744dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"OPENAI_KEY\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"OPENAI_ENDPOINT\"\n",
    "deployment_name=\"DEPLOYMENT_NAME\"\n",
    "api_version=\"API_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16ef57",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c0abf-7520-474f-83cf-09b52a1f94e1",
   "metadata": {},
   "source": [
    "1. Load: First we need to load our data. This is done with Document Loaders.\n",
    "2. Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "3. Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "[ref](https://python.langchain.com/docs/tutorials/rag/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa292002-9fa0-4904-9210-bf3476310ddb",
   "metadata": {},
   "source": [
    "![title](./imgs/index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48ed22-41d4-455b-9b44-3296e18b405d",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './inputs/'\n",
    "docs = []\n",
    "\n",
    "files = os.listdir(path)\n",
    "files = [x for x in files if x.endswith('.pdf')]\n",
    "\n",
    "for file in files:\n",
    "    loader = PyMuPDFLoader(f\"{path}/{file}\")\n",
    "    doc = loader.load()\n",
    "    for _ in doc:\n",
    "        additional_metadata = {\n",
    "                                \"last_modified_date\": file.split('.')[0].split('_')[1],\n",
    "                                \"document_name\": file.split('.')[0].split('_')[0],\n",
    "                            }\n",
    "        _.metadata.update(additional_metadata)\n",
    "\n",
    "    docs = docs + doc\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e61d83",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chucking: Split the text into chunks\n",
    "CHUNK_SIZE = 4000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    add_start_index=True\n",
    ")\n",
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"splitted texts with length: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee00c2-b16c-46f8-8766-b71242d3ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6356b0-9008-4a8b-923a-b30f3e6caf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[50].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc803bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[50].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2d676-ba45-4981-917b-37afe93f2c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[4].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a3258-dded-431c-b9df-feadce17eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[5].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dad07-71ba-40f0-a14f-9352d4186ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[4].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7170374-ffca-47e5-b53b-a29ed6d46adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[5].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19483a00",
   "metadata": {},
   "source": [
    "### Store to Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change device type to cpu if you are running on laptop\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=texts[:], embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205fd8e-4752-42ef-a86b-8ed104b20fb4",
   "metadata": {},
   "source": [
    "## Retrieval and generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea13320-7a7d-4a0b-bb5a-6c1013531a17",
   "metadata": {},
   "source": [
    "1. Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "2. Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data\n",
    "\n",
    "[Ref](https://python.langchain.com/docs/tutorials/rag/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd52201-3ef8-4a55-8a4b-848fd61c748b",
   "metadata": {},
   "source": [
    "![title](./imgs/retrieval.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f1323d-a757-45bd-ab80-524e226eaa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is SCBX's 2025 Sustainability Targets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdcfc2-cd6e-4628-8f8f-9883a32afb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7680e3-612c-414e-8d8d-964bf6b2c5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8399038-e940-436e-bebd-1e2c3d86dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956e1ab-3867-4b1e-97f3-3ce77c9e8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7221c3d6-3045-4768-abe3-8b6dbf156452",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "    Use the following context (delimited by <ctx></ctx>) to answer the question. \n",
    "    Use the context to provide the answer only. \n",
    "    ------\n",
    "    <ctx>\n",
    "    {context}\n",
    "    </ctx>\n",
    "    ------\n",
    "    {question}\n",
    "    Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=deployment_name,\n",
    "    api_version=api_version, \n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Replace with ChatOpenAI if you have access to OpenAI API\n",
    "# llm = ChatOpenAI(model_name='gpt-4o', temperature=0, streaming=True, api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae098c62-d27a-42cf-b447-cd529caa4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We’ll use the LCEL Runnable protocol to define the chain, allowing us to\n",
    "\n",
    "pipe together components and functions in a transparent way\n",
    "automatically trace our chain in LangSmith\n",
    "get streaming, async, and batched calling out of the box.\n",
    "\"\"\"\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | custom_rag_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee844d-904a-4060-866e-609b113164ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the reduction in scope 1 and scope 2 emissions that SCB achieved in 2023?\",\n",
    "    \"What share of SCBX's total revenue was powered by AI in 2023?\",\n",
    "    \"What is SCBX's 2025 financial support target for 'Net Zero financed emissions' for scope 3 emissions?\",\n",
    "    \"Can you summarize what SCBX is doing to improve financial and digital literacy?\",\n",
    "    \"What has been Thailand's share of economic loss from extreme climate events between 2000 to 2019?\",\n",
    "    \"What are SCBX's scope 1 and 2 emissions for year 2023 and how much reduction have we seen from the year before?\",\n",
    "    \"What is SCBX's scope 3 emissions baseline, which year was it measured in, and what are the top 3 sectors that fall under this category?\"\n",
    "]\n",
    "\n",
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff6ddc-ef4e-4a5e-acd3-a483f1bad4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    answer = chain.invoke(question)\n",
    "    print(answer)\n",
    "    result.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69adcbe0-6a38-4ae0-b5ae-9d33511304d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(\"what is SCBX's 2025 Sustainability Targets\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3480f-6eff-4ed3-b7c1-30eabfe56abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    \"question\": questions,\n",
    "    \"answers\": result\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42106139-4cc0-46fc-b733-81b5a0550dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7daaa9-194a-4a35-bff2-1ef14b5b5e95",
   "metadata": {},
   "source": [
    "## Return Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fb3e8-04e4-4ed9-877b-6bdb37cdd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "    Use the following context (delimited by <ctx></ctx>) to answer the question. \n",
    "    Use the context to provide the answer only. \n",
    "    ------\n",
    "    <ctx>\n",
    "    {context}\n",
    "    </ctx>\n",
    "    ------\n",
    "    {question}\n",
    "    Answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef442b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"question\": lambda x: x[\"question\"],  # input query\n",
    "        \"context\": lambda x: format_docs(x[\"context\"]),  # context\n",
    "    }\n",
    "    | custom_rag_prompt  \n",
    "    | llm  \n",
    "    | StrOutputParser()  \n",
    ")\n",
    "\n",
    "retrieve_docs = (lambda x: x[\"question\"]) | retriever\n",
    "\n",
    "# Below, we chain `.assign` calls. This takes a dict and successively\n",
    "# adds keys-- \"context\" and \"answer\"-- where the value for each key\n",
    "# is determined by a Runnable. The Runnable operates on all existing\n",
    "# keys in the dict.\n",
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"What is SCBX's financed emissions baseline, which year was it measured in?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be73ce-0907-4e02-8cc4-9b0e12577773",
   "metadata": {},
   "source": [
    "# Filter by Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f85a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_RAG_retrieval(input_pdf_path = './inputs/'):\n",
    "    docs = []\n",
    "    files = os.listdir(input_pdf_path)\n",
    "    files = [x for x in files if x.endswith('.pdf')]\n",
    "    \n",
    "    for file in files:\n",
    "        loader = PyMuPDFLoader(f\"{input_pdf_path}/{file}\")\n",
    "        doc = loader.load()\n",
    "        for _ in doc:\n",
    "            additional_metadata = {\n",
    "                                    \"last_modified_date\": file.split('.')[0].split('_')[1],\n",
    "                                    \"document_name\": file.split('.')[0].split('_')[0],\n",
    "                                }\n",
    "            _.metadata.update(additional_metadata)\n",
    "    \n",
    "        docs = docs + doc\n",
    "    \n",
    "    # Chucking: Split the text into chunks\n",
    "    CHUNK_SIZE = 4000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "    print(f\"splitted texts with length: {len(texts)}\")\n",
    "    \n",
    "    ## to replace with OpenAIEmbeddings if you have access to OpenAI API\n",
    "    # embeddings = OpenAIEmbeddings(\n",
    "    #     model='text-embedding-ada-002',\n",
    "    #     deployment='text-embedding-ada-002',\n",
    "    # )\n",
    "    \n",
    "    vectorstore = Chroma.from_documents(documents=texts[:], embedding=embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "    return vectorstore, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ec9ef-b824-4563-bdf8-7f3071c704e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_rag(retriever):\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "        Use the following context (delimited by <ctx></ctx>) to answer the question. \n",
    "        Use the context to provide the answer only. \n",
    "        ------\n",
    "        <ctx>\n",
    "        {context}\n",
    "        </ctx>\n",
    "        ------\n",
    "        {question}\n",
    "        Answer:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    custom_rag_prompt = PromptTemplate.from_template(template=PROMPT_TEMPLATE)\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | custom_rag_prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "rag_vectorstore, rag_retrieval = init_RAG_retrieval(input_pdf_path = './inputs/multiple_pdfs/')\n",
    "rag_chain = create_custom_rag(rag_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27720bd8-bb44-450d-a26a-3e4a2dabd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is SCBX's financed emissions baseline, which year was it measured in?\"\n",
    "relevant_documents = ['SCBX_Sustainability Report 2023']\n",
    "\n",
    "# filter for relevant documents\n",
    "search_kwargs = {\"k\": 5}\n",
    "retriever = rag_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs=search_kwargs)\n",
    "\n",
    "# retrieve relevant documents\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "retrieved_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c8e10-173f-4971-ad69-adb8018849be",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is SCBX's financed emissions baseline, which year was it measured in?\"\n",
    "relevant_documents = ['SCBX Sustainability Report']\n",
    "\n",
    "# filter for relevant documents\n",
    "search_kwargs = {\"k\": 5, \"filter\": {'document_name': {'$in': relevant_documents}}}\n",
    "retriever = rag_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs=search_kwargs)\n",
    "\n",
    "# retrieve relevant documents\n",
    "retrieved_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a491e6-961d-4157-b2d1-2328cca12794",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2961344b",
   "metadata": {},
   "source": [
    "## Leverage LLM Classification to build the router chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17409c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13153c9-3162-42a6-85d6-ef9433f62c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_classification = [\n",
    "  {\n",
    "    \"description\": \"SCBX sustainability, emissions baseline\",\n",
    "    \"relevant_documents\": [ \"SCBX Sustainability Report\" ]\n",
    "  },\n",
    "  {\n",
    "    \"description\": \"questions about Thailand economy, company overview for SCBX, key insights, and 2024 outlook\",\n",
    "    \"relevant_documents\" : [ \"SCBX SET Thailand Focus\" ],\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5963406d-5b0c-4eef-86a7-0d73ad878907",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = []\n",
    "\n",
    "for item in question_classification:\n",
    "    prompt_template = item[\"description\"]\n",
    "    prompt_templates.append(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25059a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "def prompt_router(input):\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    print(f\"The most similar prompt is {similarity.argmax()}\")\n",
    "    return similarity.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_router({\"query\": \"What is SCBX's financed emissions baseline, which year was it measured in?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f6131-e1a5-4dd5-918f-4fc36ac0bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_router({\"query\": \"what is the executive summary about Thailand's economy\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e443b4-cfb1-43f5-b59d-0745cc1b0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_rag(query):\n",
    "    router_idx = prompt_router({\"query\": query})\n",
    "    relevant_documents = question_classification[router_idx].get(\"relevant_documents\")\n",
    "    \n",
    "    # filter for relevant documents\n",
    "    search_kwargs = {\"k\": 5, \"filter\": {'document_name': {'$in': relevant_documents}}}\n",
    "    retriever = rag_vectorstore.as_retriever(search_type=\"similarity\", search_kwargs=search_kwargs)\n",
    "    \n",
    "    # retrieve relevant documents\n",
    "    rag_chain = create_custom_rag(retriever)\n",
    "    \n",
    "    answer = rag_chain.invoke(query)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12573b30-03ff-48ae-944d-5f333595a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_rag(query = \"What is SCBX's financed emissions baseline, which year was it measured in?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd15fad-da87-430c-a2b4-91b0bef9086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_rag(query = \"what is the executive summary about Thailand's economy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f733cf-a5dd-4e65-bc62-a44d9896e719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scbx",
   "language": "python",
   "name": "scbx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
